# Module 17 Class 2: Confusion Matrices, Decision Trees, and Ensemble Learning

## Overview

Today you will pick up where they left off on Day 1 and determine the performance of machine learning models by using a confusion matrix. Then, you will scale and normalize data across columns to avoid bias and use decision trees to make predictions. Finally, you will learn and use ensemble machine learning methods &mdash; combining multiple models to help improve the accuracy, robustness, and performance of the model.


## Learning Objectives

By the end of class, you will be able to:

* Quantify and evaluate classification models by using confusion matrices
* Interpret confusion matrices and accuracy measures 
* Use decision trees and random forests to aid the feature-selection process
* Apply bagging and boosting methods to create and use ensemble algorithms


## How does this tie into the modules?
The activities in this class will complement Lessons **17.4.1: Assess Accuracy, Precision, and Sensitivity** through **17.10.3: Combination Sampling With SMOTEENN**. You will benefit from these activities if theyâ€˜ve progressed through these lessons, which cover the following concepts, techniques, and tasks:  

* Understanding, applying, and interpreting a confusion matrix 
* Understanding and applying support vector machines
* Transforming categorical data to numerical data
* Scaling and normalizing data 
* Understanding decision trees
* Applying a decision tree classifier model
* Understanding ensemble learning methods
* Applying a random forest classifier model

## Where in the modules can I go for assistance?

  * The confusion matrix and classification reports were covered in **Lesson 17.4.1** and **Lesson 17.4.2**.
  * Decision trees were covered in **Lesson 17.7.1**.
  * Ensemble learning using `RandomForestClassifier()` was covered in **Lessons 17.8.1 - 17.8.3**.
  * Bootstrap aggregation was covered in **Lesson 17.9.1**.
  * Adaptive boosting was covered in **Lesson 17.9.2**.