# Module 16 Class 2: ETL with Amazon Web Services

## Overview

Before class, walk through this week's Challenge assignment in office hours. In class, the students will get introduced to Amazon Web Services, or AWS. They will learn how to store data on AWS Simple Storage Service, or S3, and use Postgres on their Relational Database Services, or RDS. Students will also learn how to monitor costs and shut down services to not accrue anything on top of the free-tier plan. Today's lesson will have students following along with you a bit more than usual as they get acquainted with Amazon Web Services.

## Learning Objectives

By the end of class, students will be able to:

* Create a database in AWS RDS
* Create buckets and load objects into S3
* Connect to the RDS with pgAdmin and perform Create, Read, Update, Delete (CRUD) operations.
* Use AWS for ETL



## How does this tie into the modules?
* The activities in this class will complement Lessons **16.7.1: Evaluate Amazon Web Services** through **16.9.3: Check AWS billing**.  You will benefit from these activities if theyâ€˜ve progressed through these lessons, which cover the following concepts, techniques, and tasks:

* Big data overview
* Spark architecture
* Google Colab Notebooks
* Spark DataFrames and Datasets
* Spark transformations and actions

## Where in the modules can I go for assistance?

 * Using AWS was covered in **Lesson 16.7.1**.
 * Creating an RDS instance was covered in **Lesson 16.7.2**
 * Connecting pgAdmin to an RDS instance was covered in **Lesson 16.7.3**.
 * CRUD functions were covered in **Lesson 16.7.4**.
 * AWS S3 was covered in **Lesson 16.8.2** and **Lesson 16.8.3**
 * PySpark ETL with AWS was covered in **Lesson 16.9.1**
 * AWS Cleanup and managing billing were covered in **Lesson 16.9.2** and **Lesson 16.9.3**